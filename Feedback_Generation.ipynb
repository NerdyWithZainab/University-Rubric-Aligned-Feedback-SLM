{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c166cca3",
   "metadata": {},
   "source": [
    " # University Rubric-Aligned Feedback Generation using Unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba600f8",
   "metadata": {},
   "source": [
    "This notebook combines the complete pipeline for:\n",
    " 1. Generating synthetic training data\n",
    " 2. Preparing data for fine-tuning\n",
    " 3. Fine-tuning Phi-3 with LoRA using Unsloth\n",
    " 4. Testing and evaluating the model\n",
    "\n",
    " **Requirements:**\n",
    " ```bash\n",
    " pip install unsloth transformers datasets trl scikit-learn accelerate bitsandbytes\n",
    " ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0891b587",
   "metadata": {},
   "source": [
    "### 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfd61d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import torch\n",
    "from torch.cuda import is_available\n",
    "from datasets import Dataset, load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Unsloth imports\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"‚úì All imports successful\")\n",
    "print(f\"‚úì Using device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4764f0",
   "metadata": {},
   "source": [
    "### 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9897fe27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Data generation\n",
    "    NUM_QUESTIONS = 4\n",
    "    INSTANCES_PER_QUESTION = 150\n",
    "    OUTPUT_DIR = \"output\"\n",
    "    DATA_PROCESSED_DIR = \"./data_processed\"\n",
    "    \n",
    "    # Model settings\n",
    "    model_name = \"unsloth/Phi-3-mini-4k-instruct\"\n",
    "    max_seq_length = 2048\n",
    "    load_in_4bit = True\n",
    "    \n",
    "    # LoRA settings\n",
    "    lora_r = 16\n",
    "    lora_alpha = 16\n",
    "    lora_dropout = 0\n",
    "    \n",
    "    # Training settings\n",
    "    learning_rate = 2e-4\n",
    "    num_epochs = 3\n",
    "    per_device_train_batch_size = 2\n",
    "    per_device_eval_batch_size = 2\n",
    "    gradient_accumulation_steps = 4\n",
    "    warmup_steps = 10\n",
    "    \n",
    "    # Model output\n",
    "    output_model_dir = './feedback_model_phi3'\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps = 10\n",
    "    eval_steps = 50\n",
    "    save_steps = 50\n",
    "    save_total_limit = 3\n",
    "    \n",
    "    # Evaluation\n",
    "    test_split_size = 0.15\n",
    "    val_split_size = 0.15\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(config.DATA_PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "print(\"‚úì Configuration loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b3c7d",
   "metadata": {},
   "source": [
    "### 3. Data Generation - Synthetic Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d561bb",
   "metadata": {},
   "source": [
    "#### 3.1 Define Questions and Rubrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfb0be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTIONS = [\n",
    "    {\n",
    "        \"id\": \"q1\",\n",
    "        \"course\": \"Intro to Machine Learning\",\n",
    "        \"prompt\": \"What is overfitting in machine learning, and how can cross-validation help reduce it? (2-3 sentences)\",\n",
    "        \"seeds\": [\n",
    "            \"Overfitting happens when a model learns noise or patterns specific to the training data and fails to generalize to new data. Cross-validation helps by splitting data and validating across folds so we can detect models that don't generalize and select hyperparameters accordingly.\",\n",
    "            \"When a model performs well on training data but poorly on unseen data, it is overfitting. Cross-validation estimates performance on unseen data and helps choose models or regularization settings to avoid overfitting.\"\n",
    "        ],\n",
    "        \"mistakes\": [\n",
    "            \"Describes overfitting vaguely as 'model does bad on test' without saying it learns noise.\",\n",
    "            \"Mentions cross-validation but says it 'reduces training error' rather than measuring generalization.\",\n",
    "            \"Confuses cross-validation with data augmentation or early stopping.\",\n",
    "            \"Gives only definition, no method to reduce it.\"\n",
    "        ],\n",
    "        \"rubric\": [\n",
    "            {\"code\": \"C1\", \"criterion\": \"Definition correctness\", \"max_score\": 5, \"description\": \"Clear definition describing learning noise/poor generalization\"},\n",
    "            {\"code\": \"C2\", \"criterion\": \"Cross-validation explanation\", \"max_score\": 5, \"description\": \"Explains how cross-validation helps detect or reduce overfitting\"},\n",
    "            {\"code\": \"C3\", \"criterion\": \"Conciseness and clarity\", \"max_score\": 5, \"description\": \"Answer is concise (2-3 sentences) and uses correct terminology\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q2\",\n",
    "        \"course\": \"Data Structures\",\n",
    "        \"prompt\": \"Briefly compare BFS and DFS. When would you use one over the other? (2-3 sentences)\",\n",
    "        \"seeds\": [\n",
    "            \"BFS explores neighbors level by level and is useful for finding shortest paths in unweighted graphs. DFS goes deep along a branch before backtracking and is useful for topological ordering or searching for any path when memory is limited.\",\n",
    "            \"Use BFS when you need shortest path or level information; use DFS for tasks like cycle detection or when you want to explore deep structure with less memory overhead.\"\n",
    "        ],\n",
    "        \"mistakes\": [\n",
    "            \"Says BFS is always faster than DFS or vice versa.\",\n",
    "            \"Mixes up use-cases (e.g., says DFS finds shortest paths).\",\n",
    "            \"Only describes one algorithm but not the other.\"\n",
    "        ],\n",
    "        \"rubric\": [\n",
    "            {\"code\": \"C1\", \"criterion\": \"Algorithm characteristics\", \"max_score\": 5, \"description\": \"Mentions traversal order and core property (level-order vs depth)\"},\n",
    "            {\"code\": \"C2\", \"criterion\": \"Use-case justification\", \"max_score\": 5, \"description\": \"Gives correct reasons for choosing BFS or DFS\"},\n",
    "            {\"code\": \"C3\", \"criterion\": \"Concise comparison\", \"max_score\": 5, \"description\": \"Clear, short comparison in 2-3 sentences\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q3\",\n",
    "        \"course\": \"Databases\",\n",
    "        \"prompt\": \"What does ACID mean in databases? Provide a short explanation of each property. (List and 1-line explanation)\",\n",
    "        \"seeds\": [\n",
    "            \"ACID stands for Atomicity, Consistency, Isolation, Durability. Atomicity means transactions are all-or-nothing; Consistency means DB moves between valid states; Isolation ensures concurrent transactions don't interfere; Durability ensures committed changes persist.\",\n",
    "            \"Atomicity: either all operations of a transaction happen or none. Consistency: DB constraints hold before and after transactions. Isolation: concurrent transactions appear serial. Durability: once committed, data survive crashes.\"\n",
    "        ],\n",
    "        \"mistakes\": [\n",
    "            \"Mixes up Isolation and Consistency, or gives vague descriptions.\",\n",
    "            \"Forgets one of the properties.\",\n",
    "            \"Gives overly technical answer beyond short scope.\"\n",
    "        ],\n",
    "        \"rubric\": [\n",
    "            {\"code\": \"C1\", \"criterion\": \"Coverage of properties\", \"max_score\": 5, \"description\": \"Lists all four ACID properties correctly\"},\n",
    "            {\"code\": \"C2\", \"criterion\": \"Correctness of explanations\", \"max_score\": 5, \"description\": \"Each property is explained correctly in one line\"},\n",
    "            {\"code\": \"C3\", \"criterion\": \"Brevity and clarity\", \"max_score\": 5, \"description\": \"Concise list-style explanations\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"q4\",\n",
    "        \"course\": \"Computer Networks\",\n",
    "        \"prompt\": \"Explain the TCP three-way handshake in one or two sentences.\",\n",
    "        \"seeds\": [\n",
    "            \"TCP uses SYN, SYN-ACK, ACK messages: client sends SYN, server replies SYN-ACK, client replies ACK, establishing a reliable connection. This ensures both sides are ready and agree on initial sequence numbers.\",\n",
    "            \"A three-way handshake is: client SYN, server SYN-ACK, client ACK; it's used to synchronize sequence numbers and establish a TCP connection.\"\n",
    "        ],\n",
    "        \"mistakes\": [\n",
    "            \"Says handshake uses SYN, ACK only (missing SYN-ACK).\",\n",
    "            \"Confuses UDP with TCP.\",\n",
    "            \"Mentions extra steps not part of the handshake.\"\n",
    "        ],\n",
    "        \"rubric\": [\n",
    "            {\"code\": \"C1\", \"criterion\": \"Sequence correctness\", \"max_score\": 5, \"description\": \"Mentions SYN, SYN-ACK, ACK in the right order\"},\n",
    "            {\"code\": \"C2\", \"criterion\": \"Purpose explanation\", \"max_score\": 5, \"description\": \"Explains why it's done (sync seq numbers, ensure readiness)\"},\n",
    "            {\"code\": \"C3\", \"criterion\": \"Conciseness\", \"max_score\": 5, \"description\": \"Explanation within 1-2 short sentences\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"‚úì Defined {len(QUESTIONS)} questions with rubrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce35d4d8",
   "metadata": {},
   "source": [
    "#### 3.2 Data Generation Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17301fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grade category distribution\n",
    "GRADE_CATEGORIES = [\n",
    "    (\"excellent\", 0.7),\n",
    "    (\"good\", 0.15),\n",
    "    (\"fair\", 0.08),\n",
    "    (\"poor\", 0.05),\n",
    "    (\"incorrect\", 0.02)\n",
    "]\n",
    "\n",
    "# Feedback templates\n",
    "FEEDBACK_TEMPLATES = {\n",
    "    \"definition_missing\": [\n",
    "        \"You described the topic but missed the key definition ‚Äî make sure to include that next time.\",\n",
    "        \"Good attempt, but the core definition was missing or unclear. Add a clear definition for full marks.\"\n",
    "    ],\n",
    "    \"partial_correct\": [\n",
    "        \"You partially answered the question ‚Äî correct on some points but missed: {missed}.\",\n",
    "        \"Partly correct; to improve, expand on: {missed}.\"\n",
    "    ],\n",
    "    \"minor_fix\": [\n",
    "        \"Small fix needed: {fix}. Then your answer will be complete.\",\n",
    "        \"Minor correction ‚Äî {fix}. Good otherwise.\"\n",
    "    ],\n",
    "    \"excellent_short\": [\n",
    "        \"Clear and correct‚Äîwell done.\",\n",
    "        \"Excellent answer; concise and accurate.\"\n",
    "    ],\n",
    "    \"incorrect_short\": [\n",
    "        \"The answer is incorrect or misunderstands the concept. Review {topic} and try again.\",\n",
    "        \"Incorrect: there is a misunderstanding about {topic}. Revise the core concept.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def uid():\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "def clamp(x, a, b):\n",
    "    return max(a, min(b, x))\n",
    "\n",
    "def jitter_score(base, jitter=1, min_score=0, max_score=5):\n",
    "    return clamp(int(round(base + random.uniform(-jitter, jitter))), min_score, max_score)\n",
    "\n",
    "def pick_grade_category():\n",
    "    r = random.random()\n",
    "    cum = 0\n",
    "    for cat, prob in GRADE_CATEGORIES:\n",
    "        cum += prob\n",
    "        if r <= cum:\n",
    "            return cat\n",
    "    return GRADE_CATEGORIES[-1][0]\n",
    "\n",
    "# Simple text transformation helpers\n",
    "def paraphrase(s):\n",
    "    parts = s.split(',')\n",
    "    if len(parts) > 1 and random.random() < 0.6:\n",
    "        random.shuffle(parts)\n",
    "        s = ','.join(p.strip() for p in parts)\n",
    "    s = s.replace('model', random.choice(['classifier', 'fit model']))\n",
    "    s = s.replace('training data', random.choice(['the data used for training', 'training set']))\n",
    "    return s\n",
    "\n",
    "def shorten(s):\n",
    "    if '.' in s:\n",
    "        return s.split('.')[0] + '.'\n",
    "    return s\n",
    "\n",
    "def remove_detail(s):\n",
    "    s = s.replace('so we can detect models that don\\'t generalize and select hyperparameters accordingly', '')\n",
    "    s = s.replace('and helps choose models or regularization settings to avoid overfitting', '')\n",
    "    return s\n",
    "\n",
    "def swap_keywords(s):\n",
    "    s = s.replace('cross-validation', 'data augmentation')\n",
    "    s = s.replace('SYN-ACK', 'ACK')\n",
    "    return s\n",
    "\n",
    "def mutate_answer(seed, mistakes, category):\n",
    "    \"\"\"Generate realistic student answer based on grade category\"\"\"\n",
    "    if category == \"excellent\":\n",
    "        s = seed\n",
    "        if random.random() < 0.3:\n",
    "            s = paraphrase(seed)\n",
    "        return s\n",
    "    \n",
    "    if category == \"good\":\n",
    "        s = paraphrase(seed)\n",
    "        if random.random() < 0.5 and mistakes:\n",
    "            s = remove_detail(s)\n",
    "            s += \" \" + random.choice(mistakes)\n",
    "        return s\n",
    "    \n",
    "    if category == \"fair\":\n",
    "        s = paraphrase(seed)\n",
    "        s = shorten(s)\n",
    "        if random.random() < 0.6 and mistakes:\n",
    "            s += \" \" + random.choice(mistakes)\n",
    "        return s\n",
    "    \n",
    "    if category == \"poor\":\n",
    "        s = random.choice(mistakes) if mistakes else shorten(seed)\n",
    "        if random.random() < 0.4:\n",
    "            s = \"I think \" + s\n",
    "        return s\n",
    "    \n",
    "    if category == \"incorrect\":\n",
    "        s = random.choice(mistakes) if mistakes else \"Incorrect description\"\n",
    "        s = swap_keywords(s)\n",
    "        return s\n",
    "    \n",
    "    return seed\n",
    "\n",
    "def score_from_category(category, rubric):\n",
    "    \"\"\"Generate scores based on grade category\"\"\"\n",
    "    mapping = {\n",
    "        'excellent': 0.9,\n",
    "        'good': 0.75,\n",
    "        'fair': 0.5,\n",
    "        'poor': 0.25,\n",
    "        'incorrect': 0.05\n",
    "    }\n",
    "    base_frac = mapping.get(category, 0.5)\n",
    "    scores = {}\n",
    "    for crit in rubric:\n",
    "        base = base_frac * crit['max_score']\n",
    "        scores[crit['code']] = jitter_score(base, jitter=1.2, min_score=0, max_score=crit['max_score'])\n",
    "    return scores\n",
    "\n",
    "def generate_feedback_text(scores, rubric, question_prompt, student_answer, category):\n",
    "    \"\"\"Generate rubric-aligned feedback\"\"\"\n",
    "    low = []\n",
    "    fixes = []\n",
    "    \n",
    "    for crit in rubric:\n",
    "        code = crit['code']\n",
    "        sc = scores.get(code, 0)\n",
    "        if sc <= max(1, int(0.3 * crit['max_score'])):\n",
    "            low.append(crit['criterion'])\n",
    "        elif sc < crit['max_score'] and sc < int(0.7 * crit['max_score']):\n",
    "            fixes.append(crit['criterion'])\n",
    "    \n",
    "    if category == 'excellent' and not low:\n",
    "        short = random.choice(FEEDBACK_TEMPLATES['excellent_short'])\n",
    "        detail = \"\".join([\"Great: \"+c+\". \" for c in [r['criterion'] for r in rubric]])\n",
    "        return short, detail\n",
    "    \n",
    "    if category == 'incorrect' or (len(low) == len(rubric)):\n",
    "        short = random.choice(FEEDBACK_TEMPLATES['incorrect_short']).format(topic=question_prompt.split(':')[0])\n",
    "        detail = \"You should review the core concepts and definitions for this topic.\"\n",
    "        return short, detail\n",
    "    \n",
    "    parts = []\n",
    "    if low:\n",
    "        miss = ', '.join(low[:2])\n",
    "        parts.append(random.choice(FEEDBACK_TEMPLATES['partial_correct']).format(missed=miss))\n",
    "    if fixes:\n",
    "        fix = ', '.join(fixes[:2])\n",
    "        parts.append(random.choice(FEEDBACK_TEMPLATES['minor_fix']).format(fix=fix))\n",
    "    \n",
    "    short = ' '.join(parts) if parts else random.choice(FEEDBACK_TEMPLATES['excellent_short'])\n",
    "    detail = \"Details: \" + \"; \".join([f\"{c['code']}({c['criterion']}): {scores[c['code']]}/{c['max_score']}\" for c in rubric])\n",
    "    \n",
    "    return short, detail\n",
    "\n",
    "print(\"‚úì Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d999059",
   "metadata": {},
   "source": [
    "#### 3.3 Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2f9dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(questions, instances_per_q):\n",
    "    \"\"\"Generate synthetic dataset\"\"\"\n",
    "    records = []\n",
    "    for q in questions[:config.NUM_QUESTIONS]:\n",
    "        for i in range(instances_per_q):\n",
    "            category = pick_grade_category()\n",
    "            seed = random.choice(q['seeds'])\n",
    "            ans = mutate_answer(seed, q.get('mistakes', []), category)\n",
    "            scores = score_from_category(category, q['rubric'])\n",
    "            short_fb, detailed_fb = generate_feedback_text(scores, q['rubric'], q['prompt'], ans, category)\n",
    "            \n",
    "            record = {\n",
    "                'id': uid(),\n",
    "                'question_id': q['id'],\n",
    "                'course': q['course'],\n",
    "                'prompt': q['prompt'],\n",
    "                'student_answer': ans,\n",
    "                'grade_category': category,\n",
    "                'rubric_scores': scores,\n",
    "                'rubric': q['rubric'],\n",
    "                'instructor_feedback': {\n",
    "                    'short_comment': short_fb,\n",
    "                    'detailed_comment': detailed_fb,\n",
    "                    'overall_score': sum(scores.values()),\n",
    "                    'max_overall': sum([c['max_score'] for c in q['rubric']])\n",
    "                },\n",
    "                'created_at': datetime.utcnow().isoformat() + 'Z'\n",
    "            }\n",
    "            records.append(record)\n",
    "    return records\n",
    "\n",
    "# Generate the dataset\n",
    "print(\"Generating synthetic dataset...\")\n",
    "records = build_dataset(QUESTIONS, config.INSTANCES_PER_QUESTION)\n",
    "\n",
    "# Save to JSON\n",
    "out_json = os.path.join(config.OUTPUT_DIR, 'records.json')\n",
    "with open(out_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Save sample CSV\n",
    "out_csv = os.path.join(config.OUTPUT_DIR, 'records_sample.csv')\n",
    "sample_fields = ['id', 'question_id', 'course', 'student_answer', 'grade_category', \n",
    "                 'instructor_feedback_short', 'overall_score', 'max_overall']\n",
    "with open(out_csv, 'w', newline='', encoding='utf-8') as csvf:\n",
    "    writer = csv.DictWriter(csvf, fieldnames=sample_fields)\n",
    "    writer.writeheader()\n",
    "    for r in records[:min(200, len(records))]:\n",
    "        writer.writerow({\n",
    "            'id': r['id'],\n",
    "            'question_id': r['question_id'],\n",
    "            'course': r['course'],\n",
    "            'student_answer': r['student_answer'][:200].replace('\\n', ' '),\n",
    "            'grade_category': r['grade_category'],\n",
    "            'instructor_feedback_short': r['instructor_feedback']['short_comment'],\n",
    "            'overall_score': r['instructor_feedback']['overall_score'],\n",
    "            'max_overall': r['instructor_feedback']['max_overall']\n",
    "        })\n",
    "\n",
    "print(f\"‚úì Generated {len(records)} training instances\")\n",
    "print(f\"‚úì Saved to {out_json}\")\n",
    "print(f\"‚úì CSV sample saved to {out_csv}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE GENERATED RECORD\")\n",
    "print(\"=\"*80)\n",
    "sample = records[0]\n",
    "print(f\"Course: {sample['course']}\")\n",
    "print(f\"Question: {sample['prompt']}\")\n",
    "print(f\"Student Answer: {sample['student_answer']}\")\n",
    "print(f\"Grade Category: {sample['grade_category']}\")\n",
    "print(f\"Feedback: {sample['instructor_feedback']['short_comment']}\")\n",
    "print(f\"Score: {sample['instructor_feedback']['overall_score']}/{sample['instructor_feedback']['max_overall']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69dd174",
   "metadata": {},
   "source": [
    "### 4. Data Preparation for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18545fb8",
   "metadata": {},
   "source": [
    "#### 4.1 Format Data for Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0905a19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_rubric_for_prompt(rubric):\n",
    "    \"\"\"Convert rubric to readable text\"\"\"\n",
    "    text = \"**Marking Rubric:**\\n\"\n",
    "    for criterion in rubric:\n",
    "        text += f\"- **[{criterion['code']}] {criterion['criterion']}** \"\n",
    "        text += f\"({criterion['max_score']} marks): {criterion['description']}\\n\"\n",
    "    return text\n",
    "\n",
    "def create_training_example(record, include_scores=False):\n",
    "    \"\"\"Convert record to training example format\"\"\"\n",
    "    rubric_text = format_rubric_for_prompt(record['rubric'])\n",
    "    \n",
    "    instruction = f\"\"\"You are a university teaching assistant providing constructive feedback on student answers.\n",
    "\n",
    "**Course:** {record['course']}\n",
    "**Question:** {record['prompt']}\n",
    "\n",
    "{rubric_text}\n",
    "**Student Answer:** {record['student_answer']}\n",
    "\n",
    "Provide brief, constructive feedback (2-3 sentences):\"\"\"\n",
    "    \n",
    "    response = record['instructor_feedback']['short_comment']\n",
    "    \n",
    "    return {\n",
    "        'instruction': instruction,\n",
    "        'response': response,\n",
    "        'metadata': {\n",
    "            'question_id': record['question_id'],\n",
    "            'course': record['course'],\n",
    "            'grade_category': record['grade_category'],\n",
    "            'overall_score': record['instructor_feedback']['overall_score'],\n",
    "            'max_score': record['instructor_feedback']['max_overall']\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Convert all records to training examples\n",
    "print(\"Converting records to training format...\")\n",
    "examples = [create_training_example(r) for r in records]\n",
    "\n",
    "# Shuffle\n",
    "random.shuffle(examples)\n",
    "\n",
    "# Split into train/val/test\n",
    "train_examples, temp_examples = train_test_split(\n",
    "    examples,\n",
    "    test_size=(config.test_split_size + config.val_split_size),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "val_examples, test_examples = train_test_split(\n",
    "    temp_examples,\n",
    "    test_size=(config.test_split_size / (config.test_split_size + config.val_split_size)),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"‚úì Split: {len(train_examples)} train, {len(val_examples)} val, {len(test_examples)} test\")\n",
    "\n",
    "# Convert to HuggingFace Datasets\n",
    "train_dataset = Dataset.from_list(train_examples)\n",
    "val_dataset = Dataset.from_list(val_examples)\n",
    "test_dataset = Dataset.from_list(test_examples)\n",
    "\n",
    "# Save datasets\n",
    "train_dataset.to_json(f'{config.DATA_PROCESSED_DIR}/train.json')\n",
    "val_dataset.to_json(f'{config.DATA_PROCESSED_DIR}/val.json')\n",
    "test_dataset.to_json(f'{config.DATA_PROCESSED_DIR}/test.json')\n",
    "\n",
    "print(f\"‚úì Saved datasets to {config.DATA_PROCESSED_DIR}/\")\n",
    "\n",
    "# Show sample training example\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE TRAINING EXAMPLE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nINSTRUCTION:\")\n",
    "print(train_dataset[0]['instruction'])\n",
    "print(\"\\nRESPONSE:\")\n",
    "print(train_dataset[0]['response'])\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54480e95",
   "metadata": {},
   "source": [
    " ### 5. Model Fine-Tuning with Unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edfa7b8",
   "metadata": {},
   "source": [
    "#### 5.1 Load and Configure Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faabbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_model():\n",
    "    \"\"\"Load base model and add LoRA adapters\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"LOADING MODEL\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Load base model\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=config.model_name,\n",
    "        max_seq_length=config.max_seq_length,\n",
    "        dtype=None,\n",
    "        load_in_4bit=config.load_in_4bit,\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Loaded {config.model_name}\")\n",
    "    \n",
    "    # Add LoRA adapters\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=config.lora_r,\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "        ],\n",
    "        lora_alpha=config.lora_alpha,\n",
    "        lora_dropout=config.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        use_gradient_checkpointing=\"unsloth\",\n",
    "        random_state=3407,\n",
    "        use_rslora=False,\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì LoRA configured (r={config.lora_r}, alpha={config.lora_alpha})\")\n",
    "    \n",
    "    # Apply chat template\n",
    "    tokenizer = get_chat_template(\n",
    "        tokenizer,\n",
    "        chat_template=\"phi-3\",\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Chat template applied\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "# Load model\n",
    "model, tokenizer = setup_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62055f2a",
   "metadata": {},
   "source": [
    "#### 5.2 Format Datasets for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec2caa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples, tokenizer):\n",
    "    \"\"\"Format examples using chat template\"\"\"\n",
    "    instructions = examples[\"instruction\"]\n",
    "    responses = examples[\"response\"]\n",
    "    texts = []\n",
    "    \n",
    "    for instruction, response in zip(instructions, responses):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": instruction},\n",
    "            {\"role\": \"assistant\", \"content\": response}\n",
    "        ]\n",
    "        \n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FORMATTING DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Format datasets\n",
    "train_dataset_formatted = train_dataset.map(\n",
    "    lambda x: formatting_prompts_func(x, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "val_dataset_formatted = val_dataset.map(\n",
    "    lambda x: formatting_prompts_func(x, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"‚úì Datasets formatted for training\")\n",
    "\n",
    "# Show GPU stats\n",
    "if torch.cuda.is_available():\n",
    "    gpu_stats = torch.cuda.get_device_properties(0)\n",
    "    start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "    print(f\"\\n‚úì GPU: {gpu_stats.name}\")\n",
    "    print(f\"‚úì Max memory: {max_memory} GB\")\n",
    "    print(f\"‚úì Reserved memory: {start_gpu_memory} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb78624d",
   "metadata": {},
   "source": [
    "#### 5.3 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b27fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.output_model_dir,\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    learning_rate=config.learning_rate,\n",
    "    num_train_epochs=config.num_epochs,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    logging_steps=config.logging_steps,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=config.eval_steps,\n",
    "    save_steps=config.save_steps,\n",
    "    save_total_limit=config.save_total_limit,\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    report_to=\"none\"  # Change to \"wandb\" if you want to use Weights & Biases\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset_formatted,\n",
    "    eval_dataset=val_dataset_formatted,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=config.max_seq_length,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "    dataset_num_proc=2,\n",
    "    packing=True,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"\\nStarting training...\")\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "# Show stats\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì Time: {round(trainer_stats.metrics['train_runtime']/60, 2)} minutes\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "    print(f\"‚úì Peak GPU memory: {used_memory} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2f42b5",
   "metadata": {},
   "source": [
    "#### 5.4 Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a188ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save LoRA adapters\n",
    "model.save_pretrained(f\"{config.output_model_dir}/lora_adapters\")\n",
    "tokenizer.save_pretrained(f\"{config.output_model_dir}/lora_adapters\")\n",
    "\n",
    "print(f\"‚úì Model saved to {config.output_model_dir}/lora_adapters\")\n",
    "\n",
    "# Optionally save merged model\n",
    "model.save_pretrained_merged(\n",
    "    f\"{config.output_model_dir}/merged_model\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    ")\n",
    "\n",
    "print(f\"‚úì Merged model saved to {config.output_model_dir}/merged_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87075d7",
   "metadata": {},
   "source": [
    "### 6. Model Testing and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8310a7",
   "metadata": {},
   "source": [
    "#### 6.1 Load Fine-tuned Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25b42c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_finetuned_model(model_path=f\"{config.output_model_dir}/lora_adapters\"):\n",
    "    \"\"\"Load fine-tuned model for inference\"\"\"\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_path,\n",
    "        max_seq_length=2048,\n",
    "        dtype=None,\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    "    \n",
    "    # Enable fast inference\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    \n",
    "    print(f\"‚úì Model loaded from {model_path}\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# Load the fine-tuned model\n",
    "inference_model, inference_tokenizer = load_finetuned_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a39814",
   "metadata": {},
   "source": [
    "#### 6.2 Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5213fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feedback(model, tokenizer, question, rubric, student_answer, course=\"\"):\n",
    "    \"\"\"Generate feedback for a student answer\"\"\"\n",
    "    \n",
    "    # Format rubric\n",
    "    rubric_text = format_rubric_for_prompt(rubric)\n",
    "    \n",
    "    # Build instruction\n",
    "    instruction = f\"\"\"You are a university teaching assistant providing constructive feedback on student answers.\"\"\"\n",
    "    \n",
    "    if course:\n",
    "        instruction += f\"\\n\\n**Course:** {course}\"\n",
    "    \n",
    "    instruction += f\"\"\"\n",
    "\n",
    "**Question:** {question}\n",
    "\n",
    "{rubric_text}\n",
    "**Student Answer:** {student_answer}\n",
    "\n",
    "Provide brief, constructive feedback (2-3 sentences):\"\"\"\n",
    "    \n",
    "    # Format as chat\n",
    "    messages = [{\"role\": \"user\", \"content\": instruction}]\n",
    "    \n",
    "    # Apply chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.1,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    \n",
    "    # Decode\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract assistant's response\n",
    "    if \"<|assistant|>\" in response:\n",
    "        feedback = response.split(\"<|assistant|>\")[-1].strip()\n",
    "    else:\n",
    "        feedback = response.split(instruction)[-1].strip()\n",
    "    \n",
    "    return feedback\n",
    "\n",
    "print(\"‚úì Inference function ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50383b29",
   "metadata": {},
   "source": [
    "#### 6.3 Test on Sample Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f09d5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING ON SAMPLE EXAMPLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test on a few examples from test set\n",
    "for i in range(min(5, len(test_dataset))):\n",
    "    example = test_dataset[i]\n",
    "    \n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    \n",
    "    # Extract question (simple parsing)\n",
    "    instruction = example['instruction']\n",
    "    question_start = instruction.find(\"**Question:**\") + len(\"**Question:**\")\n",
    "    question_end = instruction.find(\"**Marking Rubric:**\")\n",
    "    question = instruction[question_start:question_end].strip()\n",
    "    \n",
    "    print(f\"Question: {question[:100]}...\")\n",
    "    print(f\"\\nExpected Feedback: {example['response']}\")\n",
    "    \n",
    "    # Generate\n",
    "    messages = [{\"role\": \"user\", \"content\": instruction}]\n",
    "    prompt = inference_tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = inference_tokenizer(prompt, return_tensors=\"pt\").to(inference_model.device)\n",
    "    outputs = inference_model.generate(\n",
    "        **inputs, max_new_tokens=256, temperature=0.7,\n",
    "        do_sample=True, repetition_penalty=1.1\n",
    "    )\n",
    "    generated = inference_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    if \"<|assistant|>\" in generated:\n",
    "        generated_feedback = generated.split(\"<|assistant|>\")[-1].strip()\n",
    "    else:\n",
    "        generated_feedback = generated.split(instruction)[-1].strip()\n",
    "    \n",
    "    print(f\"Generated Feedback: {generated_feedback}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891339b4",
   "metadata": {},
   "source": [
    "#### 6.4 Interactive Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd491b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_test(model, tokenizer):\n",
    "    \"\"\"Interactive mode to test with custom inputs\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"INTERACTIVE TESTING MODE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"Enter question and student answer to get feedback\")\n",
    "    print(\"Type 'quit' to exit\\n\")\n",
    "    \n",
    "    # Example rubric\n",
    "    example_rubric = [\n",
    "        {\n",
    "            \"code\": \"C1\",\n",
    "            \"criterion\": \"Definition correctness\",\n",
    "            \"max_score\": 5,\n",
    "            \"description\": \"Clear definition with key concepts\"\n",
    "        },\n",
    "        {\n",
    "            \"code\": \"C2\",\n",
    "            \"criterion\": \"Explanation completeness\",\n",
    "            \"max_score\": 5,\n",
    "            \"description\": \"Complete explanation of the concept\"\n",
    "        },\n",
    "        {\n",
    "            \"code\": \"C3\",\n",
    "            \"criterion\": \"Clarity and conciseness\",\n",
    "            \"max_score\": 5,\n",
    "            \"description\": \"Clear and concise answer\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    while True:\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        question = input(\"Question: \")\n",
    "        if question.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        student_answer = input(\"Student answer: \")\n",
    "        if student_answer.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        # Generate feedback\n",
    "        feedback = generate_feedback(\n",
    "            model, tokenizer,\n",
    "            question=question,\n",
    "            rubric=example_rubric,\n",
    "            student_answer=student_answer,\n",
    "            course=\"Computer Science\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüìù Generated Feedback:\\n{feedback}\\n\")\n",
    "\n",
    "# Uncomment to run interactive testing\n",
    "# interactive_test(inference_model, inference_tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388c4a7d",
   "metadata": {},
   "source": [
    "#### 6.5 Quantitative Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c23158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUANTITATIVE EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Evaluate on full test set\n",
    "results = []\n",
    "\n",
    "for i, example in enumerate(test_dataset):\n",
    "    instruction = example['instruction']\n",
    "    \n",
    "    # Generate\n",
    "    messages = [{\"role\": \"user\", \"content\": instruction}]\n",
    "    prompt = inference_tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = inference_tokenizer(prompt, return_tensors=\"pt\").to(inference_model.device)\n",
    "    outputs = inference_model.generate(\n",
    "        **inputs, max_new_tokens=256, temperature=0.7,\n",
    "        do_sample=True, repetition_penalty=1.1\n",
    "    )\n",
    "    generated = inference_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    if \"<|assistant|>\" in generated:\n",
    "        generated_feedback = generated.split(\"<|assistant|>\")[-1].strip()\n",
    "    else:\n",
    "        generated_feedback = generated.split(instruction)[-1].strip()\n",
    "    \n",
    "    results.append({\n",
    "        'example_id': i,\n",
    "        'question_id': example['metadata']['question_id'],\n",
    "        'grade_category': example['metadata']['grade_category'],\n",
    "        'expected': example['response'],\n",
    "        'generated': generated_feedback\n",
    "    })\n",
    "\n",
    "# Save results\n",
    "with open('evaluation_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Evaluated {len(results)} test examples\")\n",
    "print(f\"‚úì Results saved to evaluation_results.json\")\n",
    "\n",
    "# Show some statistics\n",
    "grade_categories = {}\n",
    "for r in results:\n",
    "    cat = r['grade_category']\n",
    "    if cat not in grade_categories:\n",
    "        grade_categories[cat] = 0\n",
    "    grade_categories[cat] += 1\n",
    "\n",
    "print(\"\\nTest set distribution:\")\n",
    "for cat, count in sorted(grade_categories.items()):\n",
    "    print(f\"  {cat}: {count} examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da322c5f",
   "metadata": {},
   "source": [
    "### 7. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523afca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéâ PIPELINE COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nWhat we accomplished:\")\n",
    "print(\"‚úì Generated synthetic dataset with rubric-aligned feedback\")\n",
    "print(\"‚úì Prepared data for fine-tuning\")\n",
    "print(\"‚úì Fine-tuned Phi-3 model using Unsloth with LoRA\")\n",
    "print(\"‚úì Evaluated model on test set\")\n",
    "print(\"\\nModel location:\", config.output_model_dir)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Review evaluation_results.json for quality assessment\")\n",
    "print(\"2. Try interactive testing with your own examples\")\n",
    "print(\"3. Experiment with different hyperparameters\")\n",
    "print(\"4. Add more questions and rubrics to expand dataset\")\n",
    "print(\"5. Implement automatic metrics (BLEU, ROUGE, BERTScore)\")\n",
    "print(\"6. Deploy model for production use\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
